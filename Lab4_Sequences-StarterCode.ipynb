{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Introduction to Mining Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will first focus on Sequence Comparison/Alignment. We will compute the Levenshtein Edit Distance for country names and implement a simple version of BLAST to compare genomes. Then, we will turn to Sequence Prediction/Labeling and will walk through a Named Entity Extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy --upgrade\n",
    "# # and then restart kernel\n",
    "# %pip install country_list\n",
    "from country_list import countries_for_language\n",
    "# %pip install Bio\n",
    "from Bio.Blast import NCBIWWW, NCBIXML # calling online version\n",
    "#%pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# %pip install pomegranate\n",
    "# %pip install pomegranate --no-cache-dir --no-binary :all:\n",
    "# may need to run 'pip install pomegranate --no-cache-dir --no-binary :all:' depending on setup\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "# if when running import, give an error saying something about numpy version:\n",
    "# %pip install sklearn_crfsuite\n",
    "import sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence Comparison/Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Levenshtein Edit Distance\n",
    "\n",
    "Levenshtein Edit Distance helps us assess the similarity of sequences. It is calculated in terms of the minimum amount of operations (insertion, deletions and substitutions) that must be made to make two sequences identical. The smaller the distance, the greater the similarity between the sequences. To show how it is computed, we can generate a matrix and find the \"trace\" or the path for the minimum edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros((size_x, size_y), dtype=\"object\") # changed dtype\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    \n",
    "    # added code for readability\n",
    "    matrix[0,0] = \"\"\n",
    "    for x in range(1, size_x):\n",
    "        matrix [x, 0] = seq1[x-1]\n",
    "    for y in range(1, size_y):\n",
    "        matrix [0, y] = seq2[y-1]\n",
    "        \n",
    "    print(matrix)\n",
    "    \n",
    "    return matrix[size_x - 1, size_y - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to test this function, let's try out the example from class to compute the distance between \"William Cohen\" and \"William Cohon\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' 'W' 'i' 'l' 'l' 'l' 'i' 'a' 'm' ' ' 'C' 'o' 'h' 'o' 'n']\n",
      " ['W' 0 1 2 3 4 5 6 7 8 9 10 11 12 13]\n",
      " ['i' 1 0 1 2 3 4 5 6 7 8 9 10 11 12]\n",
      " ['l' 2 1 0 1 2 3 4 5 6 7 8 9 10 11]\n",
      " ['l' 3 2 1 0 1 2 3 4 5 6 7 8 9 10]\n",
      " ['i' 4 3 2 1 1 1 2 3 4 5 6 7 8 9]\n",
      " ['a' 5 4 3 2 2 2 1 2 3 4 5 6 7 8]\n",
      " ['m' 6 5 4 3 3 3 2 1 2 3 4 5 6 7]\n",
      " [' ' 7 6 5 4 4 4 3 2 1 2 3 4 5 6]\n",
      " ['C' 8 7 6 5 5 5 4 3 2 1 2 3 4 5]\n",
      " ['o' 9 8 7 6 6 6 5 4 3 2 1 2 3 4]\n",
      " ['h' 10 9 8 7 7 7 6 5 4 3 2 1 2 3]\n",
      " ['e' 11 10 9 8 8 8 7 6 5 4 3 2 2 3]\n",
      " ['n' 12 11 10 9 9 9 8 7 6 5 4 3 3 2]]\n",
      "Edit Distance = 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Edit Distance =\", levenshtein(\"William Cohen\", \"Willliam Cohon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it matches the result from class. Next, let's try an example with countries. First, let's comment out the print command from the function above to reduce the size of our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros((size_x, size_y), dtype=\"object\") # changed dtype\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    \n",
    "    # added code for readability\n",
    "    matrix[0,0] = \"\"\n",
    "    for x in range(1, size_x):\n",
    "        matrix [x, 0] = seq1[x-1]\n",
    "    for y in range(1, size_y):\n",
    "        matrix [0, y] = seq2[y-1]\n",
    "        \n",
    "    #print(matrix)\n",
    "    \n",
    "    return matrix[size_x - 1, size_y - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this to work, we must also load a list of countries for reference using the country_list module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Afghanistan', 'Åland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua & Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Bosnia & Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'British Virgin Islands', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Caribbean Netherlands', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo - Brazzaville', 'Congo - Kinshasa', 'Cook Islands', 'Costa Rica', 'Côte d’Ivoire', 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czechia', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Falkland Islands', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard & McDonald Islands', 'Honduras', 'Hong Kong SAR China', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao SAR China', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar (Burma)', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'North Korea', 'North Macedonia', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territories', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn Islands', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russia', 'Rwanda', 'Samoa', 'San Marino', 'São Tomé & Príncipe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia & South Sandwich Islands', 'South Korea', 'South Sudan', 'Spain', 'Sri Lanka', 'St. Barthélemy', 'St. Helena', 'St. Kitts & Nevis', 'St. Lucia', 'St. Martin', 'St. Pierre & Miquelon', 'St. Vincent & Grenadines', 'Sudan', 'Suriname', 'Svalbard & Jan Mayen', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad & Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks & Caicos Islands', 'Tuvalu', 'U.S. Outlying Islands', 'U.S. Virgin Islands', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Vatican City', 'Venezuela', 'Vietnam', 'Wallis & Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']\n"
     ]
    }
   ],
   "source": [
    "countries = list(dict(countries_for_language('en')).values())\n",
    "print(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the country names, it is evident that some of them may be difficult to spell correctly. Therefore, one way we can use the Levenshtein Distance is to find the correct spelling of a country name by finding the one that is closest our misspelling. See below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Philippines', 2), ('Chile', 7), ('Maldives', 7)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to calculatedisrance\n",
    "def find_closest_country(country_mispelling):\n",
    "    distances = {}\n",
    "    for country in countries:\n",
    "        distance = levenshtein(country_mispelling, country)\n",
    "        distances[country] = distance\n",
    "    return sorted(distances.items(), key=lambda c:c[1])[0:3] # return the three closest countries\n",
    "\n",
    "find_closest_country(\"Phillipines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Uruguay', 3), ('Hungary', 4), ('Palau', 4)]\n",
      "[('Mauritania', 0), ('Mauritius', 4), ('Tanzania', 5)]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE: try this with other common country mispellings\n",
    "\n",
    "### for inspiration: https://www.funtrivia.com/playquiz/quiz32051824b17d8.html\n",
    "\n",
    "print(find_closest_country('Uragauy'))\n",
    "print(find_closest_country('Mauritania'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to helping identify the correct spelling of a word, Levenshtein Distance can help us with auto-completion tasks. See below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sri Lanka', 3), ('Syria', 3), ('Aruba', 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_country(\"Sri La\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 BLAST: Basic Local Alignment Search Tool\n",
    "\n",
    "We can think of BLAST as a heuristic model for performing local alignment tasks that operates by searching for high scoring segment pairs (HSPs). As discussed in lecture, BLAST views sequences as sequences of short words (k-tuple). Therefore, in the case of good alignment, we should see close matches between many of these sub-sequences, or HSPs. To showcase BLAST, we will utilize an online version provided through NCBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function qblast in module Bio.Blast.NCBIWWW:\n",
      "\n",
      "qblast(program, database, sequence, url_base='https://blast.ncbi.nlm.nih.gov/Blast.cgi', auto_format=None, composition_based_statistics=None, db_genetic_code=None, endpoints=None, entrez_query='(none)', expect=10.0, filter=None, gapcosts=None, genetic_code=None, hitlist_size=50, i_thresh=None, layout=None, lcase_mask=None, matrix_name=None, nucl_penalty=None, nucl_reward=None, other_advanced=None, perc_ident=None, phi_pattern=None, query_file=None, query_believe_defline=None, query_from=None, query_to=None, searchsp_eff=None, service=None, threshold=None, ungapped_alignment=None, word_size=None, short_query=None, alignments=500, alignment_view=None, descriptions=500, entrez_links_new_window=None, expect_low=None, expect_high=None, format_entrez_query=None, format_object=None, format_type='XML', ncbi_gi=None, results_file=None, show_overview=None, megablast=None, template_type=None, template_length=None)\n",
      "    BLAST search using NCBI's QBLAST server or a cloud service provider.\n",
      "    \n",
      "    Supports all parameters of the old qblast API for Put and Get.\n",
      "    \n",
      "    Please note that NCBI uses the new Common URL API for BLAST searches\n",
      "    on the internet (http://ncbi.github.io/blast-cloud/dev/api.html). Thus,\n",
      "    some of the parameters used by this function are not (or are no longer)\n",
      "    officially supported by NCBI. Although they are still functioning, this\n",
      "    may change in the future.\n",
      "    \n",
      "    The Common URL API (http://ncbi.github.io/blast-cloud/dev/api.html) allows\n",
      "    doing BLAST searches on cloud servers. To use this feature, please set\n",
      "    ``url_base='http://host.my.cloud.service.provider.com/cgi-bin/blast.cgi'``\n",
      "    and ``format_object='Alignment'``. For more details, please see\n",
      "    https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastDocs&DOC_TYPE=CloudBlast\n",
      "    \n",
      "    Some useful parameters:\n",
      "    \n",
      "     - program        blastn, blastp, blastx, tblastn, or tblastx (lower case)\n",
      "     - database       Which database to search against (e.g. \"nr\").\n",
      "     - sequence       The sequence to search.\n",
      "     - ncbi_gi        TRUE/FALSE whether to give 'gi' identifier.\n",
      "     - descriptions   Number of descriptions to show.  Def 500.\n",
      "     - alignments     Number of alignments to show.  Def 500.\n",
      "     - expect         An expect value cutoff.  Def 10.0.\n",
      "     - matrix_name    Specify an alt. matrix (PAM30, PAM70, BLOSUM80, BLOSUM45).\n",
      "     - filter         \"none\" turns off filtering.  Default no filtering\n",
      "     - format_type    \"HTML\", \"Text\", \"ASN.1\", or \"XML\".  Def. \"XML\".\n",
      "     - entrez_query   Entrez query to limit Blast search\n",
      "     - hitlist_size   Number of hits to return. Default 50\n",
      "     - megablast      TRUE/FALSE whether to use MEga BLAST algorithm (blastn only)\n",
      "     - short_query    TRUE/FALSE whether to adjust the search parameters for a\n",
      "                      short query sequence. Note that this will override\n",
      "                      manually set parameters like word size and e value. Turns\n",
      "                      off when sequence length is > 30 residues. Default: None.\n",
      "     - service        plain, psi, phi, rpsblast, megablast (lower case)\n",
      "    \n",
      "    This function does no checking of the validity of the parameters\n",
      "    and passes the values to the server as is.  More help is available at:\n",
      "    https://ncbi.github.io/blast-cloud/dev/api.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sources: \n",
    "# https://www.tutorialspoint.com/biopython/biopython_overview_of_blast.htm\n",
    "# https://blast.ncbi.nlm.nih.gov/Blast.cgi\n",
    "\n",
    "help(NCBIWWW.qblast) # help documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.StringIO at 0x23675c13a60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example running qblast\n",
    "\n",
    "# WARNING: CAN TAKE ~5-10 minutes to run depending on load\n",
    "\n",
    "# example of poliovirus\n",
    "# blastn = nucleotide search program\n",
    "# nt = nucleotide database \n",
    "# sequence = GI number of the sequence (poliovirus in this case) = 12408699 \n",
    "\n",
    "result_handle = NCBIWWW.qblast(\"blastn\", \"nt\", sequence = 12408699) \n",
    "result_handle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results to a file\n",
    "\n",
    "with open('polio_results.xml', 'w') as results_file: \n",
    "    results = result_handle.read() \n",
    "    results_file.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: gi|61252|emb|V01149.1| Human poliovirus 1 Mahoney, complete genome \n",
      "\n",
      "Match: gi|1012282727|gb|KU866422.1| Poliovirus 1 strain Mahoney_CDC, complete genome \n",
      "\n",
      "Match: gi|27085396|gb|AY184219.1| Human poliovirus 1 strain Sabin 1, complete genome \n",
      "\n",
      "Match: gi|283137719|gb|GQ984141.1| Human poliovirus 1 strain Sabin 1 isolate S302, complete genome \n",
      "\n",
      "Match: gi|61236|emb|V01148.1| Genome of human poliovirus type 1 (Mahoney strain). (One of two versions.) \n",
      "\n",
      "Match: gi|61257|emb|V01150.1| Human poliovirus strain Sabin 1 complete genome, strain Sabin 1 \n",
      "\n",
      "Match: gi|987390362|gb|KT353719.1| Poliovirus 1 strain 1-B2, complete genome \n",
      "\n",
      "Match: gi|18644085|emb|AJ430385.1| Human poliovirus 1 genomic RNA for polyprotein gene, strain Cox \n",
      "\n",
      "Match: gi|193245070|gb|EU794954.1| Human poliovirus 1 isolate A49 polyprotein gene, complete cds \n",
      "\n",
      "Match: gi|643433715|gb|KJ170502.1| Human poliovirus 1 strain NIE0918388, complete genome >gi|643433717|gb|KJ170503.1| Human poliovirus 1 strain NIE0918389, complete genome \n",
      "\n",
      "Match: gi|643433711|gb|KJ170500.1| Human poliovirus 1 strain NIE0918315, complete genome \n",
      "\n",
      "Match: gi|643433629|gb|KJ170459.1| Human poliovirus 1 strain NIE1118378, complete genome >gi|643433631|gb|KJ170460.1| Human poliovirus 1 strain NIE1118331, complete genome >gi|643433633|gb|KJ170461.1| Human poliovirus 1 strain NIE1118336, complete genome >gi|643433635|gb|KJ170462.1| Human poliovirus 1 strain NIE1118340, complete genome >gi|643433637|gb|KJ170463.1| Human poliovirus 1 strain NIE1218342, complete genome >gi|643433639|gb|KJ170464.1| Human poliovirus 1 strain NIE1218345, complete genome >gi|643433641|gb|KJ170465.1| Human poliovirus 1 strain NIE1218347, complete genome >gi|643433643|gb|KJ170466.1| Human poliovirus 1 strain NIE1218350, complete genome >gi|643433645|gb|KJ170467.1| Human poliovirus 1 strain NIE0918311, complete genome >gi|643433647|gb|KJ170468.1| Human poliovirus 1 strain NIE1118338, complete genome >gi|643433649|gb|KJ170469.1| Human poliovirus 1 strain NIE1218343, complete genome >gi|643433651|gb|KJ170470.1| Human poliovirus 1 strain NIE1018396, complete genome >gi|643433653|gb|KJ170471.1| Human poliovirus 1 strain NIE1118334, complete genome >gi|643433655|gb|KJ170472.1| Human poliovirus 1 strain NIE1118332, complete genome >gi|643433657|gb|KJ170473.1| Human poliovirus 1 strain NIE1118337, complete genome >gi|643433659|gb|KJ170474.1| Human poliovirus 1 strain NIE1118339, complete genome \n",
      "\n",
      "Match: gi|193245072|gb|EU794955.1| Human poliovirus 1 isolate A63 polyprotein gene, complete cds \n",
      "\n",
      "Match: gi|1402399317|gb|MG571844.1| MAG: Poliovirus 1 clone V8A1 polyprotein gene, complete cds \n",
      "\n",
      "Match: gi|643433721|gb|KJ170505.1| Human poliovirus 1 strain NIE1118317, complete genome \n",
      "\n",
      "Match: gi|643433707|gb|KJ170498.1| Human poliovirus 1 strain NIE1218353, complete genome \n",
      "\n",
      "Match: gi|643433705|gb|KJ170497.1| Human poliovirus 1 strain NIE1218344, complete genome \n",
      "\n",
      "Match: gi|643433703|gb|KJ170496.1| Human poliovirus 1 strain NIE1218346, complete genome \n",
      "\n",
      "Match: gi|643433695|gb|KJ170492.1| Human poliovirus 1 strain NIE1118400, complete genome \n",
      "\n",
      "Match: gi|643433691|gb|KJ170490.1| Human poliovirus 1 strain NIE1118333, complete genome \n",
      "\n",
      "Match: gi|643433685|gb|KJ170487.1| Human poliovirus 1 strain NIE1118318, complete genome \n",
      "\n",
      "Match: gi|643433679|gb|KJ170484.1| Human poliovirus 1 strain NIE0918312, complete genome \n",
      "\n",
      "Match: gi|643433675|gb|KJ170482.1| Human poliovirus 1 strain NIE1218341, complete genome \n",
      "\n",
      "Match: gi|643433673|gb|KJ170481.1| Human poliovirus 1 strain NIE1118330, complete genome \n",
      "\n",
      "Match: gi|643433671|gb|KJ170480.1| Human poliovirus 1 strain NIE1118328, complete genome \n",
      "\n",
      "Match: gi|643433665|gb|KJ170477.1| Human poliovirus 1 strain NIE0918314, complete genome \n",
      "\n",
      "Match: gi|643433661|gb|KJ170475.1| Human poliovirus 1 strain NIE1018320, complete genome \n",
      "\n",
      "Match: gi|643433619|gb|KJ170454.1| Human poliovirus 1 strain NIE1118401, complete genome \n",
      "\n",
      "Match: gi|643433613|gb|KJ170451.1| Human poliovirus 1 strain NIE1018321, complete genome \n",
      "\n",
      "Match: gi|643433607|gb|KJ170448.1| Human poliovirus 1 strain NIE1118376, complete genome >gi|643433609|gb|KJ170449.1| Human poliovirus 1 strain NIE1118368, complete genome \n",
      "\n",
      "Match: gi|643433727|gb|KJ170508.1| Human poliovirus 1 strain NIE1018382, complete genome \n",
      "\n",
      "Match: gi|643433713|gb|KJ170501.1| Human poliovirus 1 strain NIE1018393, complete genome \n",
      "\n",
      "Match: gi|643433687|gb|KJ170488.1| Human poliovirus 1 strain NIE1018325, complete genome \n",
      "\n",
      "Match: gi|643433683|gb|KJ170486.1| Human poliovirus 1 strain NIE0918380, complete genome \n",
      "\n",
      "Match: gi|643433681|gb|KJ170485.1| Human poliovirus 1 strain NIE1218348, complete genome \n",
      "\n",
      "Match: gi|643433677|gb|KJ170483.1| Human poliovirus 1 strain NIE1218349, complete genome \n",
      "\n",
      "Match: gi|643433669|gb|KJ170479.1| Human poliovirus 1 strain NIE1118386, complete genome \n",
      "\n",
      "Match: gi|643433667|gb|KJ170478.1| Human poliovirus 1 strain NIE0918391, complete genome \n",
      "\n",
      "Match: gi|643433663|gb|KJ170476.1| Human poliovirus 1 strain NIE1118404, complete genome \n",
      "\n",
      "Match: gi|643433627|gb|KJ170458.1| Human poliovirus 1 strain NIE1018322, complete genome \n",
      "\n",
      "Match: gi|643433623|gb|KJ170456.1| Human poliovirus 1 strain NIE1118403, complete genome \n",
      "\n",
      "Match: gi|643433617|gb|KJ170453.1| Human poliovirus 1 strain NIE1118398, complete genome \n",
      "\n",
      "Match: gi|643433615|gb|KJ170452.1| Human poliovirus 1 strain NIE1118397, complete genome \n",
      "\n",
      "Match: gi|643433605|gb|KJ170447.1| Human poliovirus 1 strain NIE1118369, complete genome \n",
      "\n",
      "Match: gi|643433603|gb|KJ170446.1| Human poliovirus 1 strain NIE1018366, complete genome \n",
      "\n",
      "Match: gi|643433593|gb|KJ170441.1| Human poliovirus 1 strain NIE1018362, complete genome >gi|643433595|gb|KJ170442.1| Human poliovirus 1 strain NIE1018364, complete genome >gi|643433597|gb|KJ170443.1| Human poliovirus 1 strain NIE1118406, complete genome >gi|643433599|gb|KJ170444.1| Human poliovirus 1 strain NIE1018361, complete genome \n",
      "\n",
      "Match: gi|643433589|gb|KJ170439.1| Human poliovirus 1 strain NIE1018354, complete genome \n",
      "\n",
      "Match: gi|643433585|gb|KJ170437.1| Human poliovirus 1 strain NIE1218351, complete genome \n",
      "\n",
      "Match: gi|193245068|gb|EU794953.1| Human poliovirus 1 isolate A21 polyprotein gene, complete cds \n",
      "\n",
      "Match: gi|643433759|gb|KJ170524.1| Human poliovirus 1 strain NIE0918390, complete genome \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look through the results\n",
    "\n",
    "threshold = 1e-20 # think of this like a p-value\n",
    "for record in NCBIXML.parse(open(\"polio_results.xml\")): \n",
    "     if record.alignments: \n",
    "        for align in record.alignments: \n",
    "            for hsp in align.hsps: \n",
    "                if hsp.expect < threshold: \n",
    "                    print(\"Match:\", align.title, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From running this query, we see that we get back a bunch of different samples of poliovirus variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE: try this out with another nucleotide sequence\n",
    "\n",
    "### inspiration: find the GI number of a sequence using https://www.ncbi.nlm.nih.gov/nuccore\n",
    "\n",
    "COVID = 1798172431\n",
    "result_handle = NCBIWWW.qblast('blastn', 'nt', sequence=1798172431)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequence Prediction/Labeling\n",
    "\n",
    "For this part of the lab, we will be experimenting with different models to perform Named Entity Recognition (NER). The goal of NER is to label items into a set of predefined states/entities, such as an organizations, dates/times, geographical locations, etc. To do so, we will use a dummy majority classifier as our baseline and then will experiment with two sequence models introduced in lecture: Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Exploration, Data Preparation & Train-Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "# data source: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n",
    "ner_dataset = pd.read_csv('ner_dataset.csv', encoding = \"ISO-8859-1\")\n",
    "print(ner_dataset.shape)\n",
    "ner_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 47959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many sentences?\n",
    "print(\"Number of sentences:\", ner_dataset['Sentence #'].nunique())\n",
    "\n",
    "# distribution of tags?\n",
    "ner_dataset['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes about the data:\n",
    "\n",
    "We can think of 'O' as background or not a named entity. We see that 'O' is by far the most common state indicating that most text is not a named entity in this case.\n",
    "\n",
    "'B-' indicates the beginning of the named entity and 'I' indicates that it is a continued part of the named entity.\n",
    "\n",
    "Overall, we see that the distribution of the named entities is heavily skewed with some entities appearing much more than others in the data. This is important to keep in mind when fitting models and assessing performance.\n",
    "\n",
    "\n",
    "Next, we will want to fill in the NaN values in the 'Sentence' column with the previous value so that we can attribute each word/tag to the correct sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in NaN sentences\n",
    "ner_dataset.fillna(method='ffill', inplace=True)\n",
    "ner_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this dataset is very large, we will only work with 2,000 sentences in our training set and 1,000 sentences in our testing set to reduce computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44396, 4)\n",
      "(44396, 3)\n",
      "(44396,)\n",
      "(21778, 4)\n",
      "(21778, 3)\n",
      "(21778,)\n"
     ]
    }
   ],
   "source": [
    "ner_train = ner_dataset[0:44396] # first 2000 sentences\n",
    "ner_test = ner_dataset[44396:66174] # next 1000 sentences\n",
    "\n",
    "ner_X_train = ner_train.drop(columns=\"Tag\") # training features\n",
    "ner_y_train = ner_train.Tag # training outcome variable\n",
    "ner_X_test = ner_test.drop(columns=\"Tag\") # testing features\n",
    "ner_y_test = ner_test.Tag # testing outcome variable\n",
    "\n",
    "# checks\n",
    "print(ner_train.shape)\n",
    "print(ner_X_train.shape)\n",
    "print(ner_y_train.shape)\n",
    "print(ner_test.shape)\n",
    "print(ner_X_test.shape)\n",
    "print(ner_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Dummy Majority Classifier\n",
    "\n",
    "The first model we will implement is a dummy classifier that will label each word with the most frequent entity, which is 'O' in this case. This will serve as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyClassifier</label><div class=\"sk-toggleable__content\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "dc = DummyClassifier(strategy=\"most_frequent\") # initalize the model\n",
    "dc.fit(ner_X_train, ner_y_train) # fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.85      1.00      0.92     18417\n",
      "       B-geo       0.00      0.00      0.00       784\n",
      "       B-gpe       0.00      0.00      0.00       346\n",
      "       B-per       0.00      0.00      0.00       410\n",
      "       I-geo       0.00      0.00      0.00       165\n",
      "       B-org       0.00      0.00      0.00       363\n",
      "       I-org       0.00      0.00      0.00       310\n",
      "       B-tim       0.00      0.00      0.00       390\n",
      "       B-art       0.00      0.00      0.00         7\n",
      "       I-art       0.00      0.00      0.00         9\n",
      "       I-per       0.00      0.00      0.00       420\n",
      "       I-gpe       0.00      0.00      0.00         4\n",
      "       I-tim       0.00      0.00      0.00       125\n",
      "       B-nat       0.00      0.00      0.00         2\n",
      "       B-eve       0.00      0.00      0.00        13\n",
      "       I-eve       0.00      0.00      0.00        13\n",
      "       I-nat       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     21778\n",
      "   macro avg       0.05      0.06      0.05     21778\n",
      "weighted avg       0.72      0.85      0.77     21778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# assess performance using classification_report\n",
    "\n",
    "states = list(ner_y_train.unique()) # list of unique states in our training dataset\n",
    "\n",
    "dc_preds = dc.predict(ner_X_test) # make predictions for test data\n",
    "print(classification_report(y_true = ner_y_test, y_pred = dc_preds, labels = states)) # assess performance by hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we see that this baseline model only performs well for 'O' since it is predicting 'O' for every observation. This shows that we must be extra careful when interpreting the results since the large representation of 'O' in the data will bias the overall results (i.e., the micro/macro/weighted avgs). This is is why we cannot just look at the overall results and must assess performance by hidden state as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 HMMs\n",
    "Next we will implement a Hidden Markov Model, or HMM. As review, for HMMs, we have (1) a set of observed states, (2) a set of hidden states, (3) initial state probabilities, (4) state transition probabilities, and (5) observation state probabilities. HMMs are a generative model meaning that we can think of the the hidden states as \"generating\" the observations. This can help explain why a particular sequence was observed by finding the most likely path that generated the observations. However, it is important to remember that with HMMs, the hidden states are only dependent on the previous state and the observed states are only dependent on its hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://towardsdatascience.com/part-of-speech-tagging-with-hidden-markov-chain-models-e9fccc835c0e\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\") # intialize our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(ner_train.Tag) # list of tags\n",
    "words = list(ner_train.Word) # list of words\n",
    "words = [word.lower() for word in words] # lowercase all words\n",
    "\n",
    "tags_count = Counter(tags) # frequency of each tag\n",
    "\n",
    "tag_combos = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)] # look at each pair of tags\n",
    "tag_bigrams = Counter(tag_combos) # count the frequency of each pair of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_tag = ner_train.drop_duplicates(subset = 'Sentence #', keep=\"first\").Tag # starting tags\n",
    "starting_tag_count = Counter(starts_tag) # frequency of each starting tag\n",
    "\n",
    "end_tag = ner_train.drop_duplicates(subset = 'Sentence #', keep=\"last\").Tag # ending tags\n",
    "ending_tag_count = Counter(end_tag) # frequency of each ending tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the frequency of each tag/word combo\n",
    "\n",
    "def pair_counts(tags, words):\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        counts[tag][word] += 1\n",
    "    return counts\n",
    "\n",
    "tag_words_count = pair_counts(tags, words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability distribution of words at each state\n",
    "\n",
    "test_words = list(ner_test.Word) # list of test words\n",
    "test_words = [word.lower() for word in test_words] # lowercase all words\n",
    "all_words = set(words + test_words) # unique set of all words in train & test combined\n",
    "\n",
    "to_pass_states = []\n",
    "for tag, words_dict in tag_words_count.items():\n",
    "    total = float(sum(words_dict.values()))\n",
    "    distribution = {word: count/total for word, count in words_dict.items()}\n",
    "    \n",
    "    # add default probability for all words (in train and/or test) to prevent errors during prediction\n",
    "    for word in all_words:\n",
    "        if word not in list(distribution.keys()):\n",
    "                distribution[word] = 1/1000\n",
    "    \n",
    "    tag_emissions = DiscreteDistribution(distribution)\n",
    "    tag_state = State(tag_emissions, name=tag)\n",
    "    to_pass_states.append(tag_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.add_states()\n",
    "\n",
    "# start probability for each state\n",
    "start_prob={}\n",
    "for tag in tags:\n",
    "    start_prob[tag] = starting_tag_count[tag]/tags_count[tag]\n",
    "    \n",
    "for tag_state in to_pass_states:\n",
    "    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])\n",
    "\n",
    "# end probability for each state\n",
    "end_prob={}\n",
    "for tag in tags:\n",
    "    end_prob[tag]=ending_tag_count[tag]/tags_count[tag]\n",
    "    \n",
    "for tag_state in to_pass_states:\n",
    "    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition probabilities between states\n",
    "\n",
    "transition_prob_pair = {}\n",
    "for key in tag_bigrams.keys():\n",
    "    transition_prob_pair[key] = tag_bigrams.get(key)/tags_count[key[0]]\n",
    "    \n",
    "for tag_state in to_pass_states :\n",
    "    for next_tag_state in to_pass_states:\n",
    "        try:\n",
    "            transition = transition_prob_pair[(tag_state.name,next_tag_state.name)]\n",
    "        except:\n",
    "            transition = 0 # if transition is not found in data\n",
    "        basic_model.add_transition(tag_state,next_tag_state,transition)\n",
    "        \n",
    "basic_model.bake() # finalize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the model, we can use it to make predictions on the test data. To do so, we will create a nested list of our test sentences where each sentence is its own list. Then, we will make our predictions on each sentence individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'wednesday', ',', 'police', 're-opened', 'a', 'central', 'square', 'in', 'the', 'city', 'after', 'clearing', 'out', 'protesters', '.']\n"
     ]
    }
   ],
   "source": [
    "# get list of test sentences\n",
    "\n",
    "sentences_dict_test = {}\n",
    "\n",
    "for index, row in ner_test.iterrows():\n",
    "    if row['Sentence #'] not in sentences_dict_test:\n",
    "        sentences_dict_test[row['Sentence #']] = []\n",
    "    sentences_dict_test[row['Sentence #']].append((row['Word'].lower()))\n",
    "    \n",
    "sentences_test = list(sentences_dict_test.values())\n",
    "print(sentences_test[1]) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions by the sentence\n",
    "\n",
    "hmm_preds = []\n",
    "for sentence in sentences_test:\n",
    "    _, state_path = basic_model.viterbi(sentence)\n",
    "    tags = [state[1].name for state in state_path[1:-1]]\n",
    "    hmm_preds.append(tags)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.90      0.80      0.85     18417\n",
      "       B-geo       0.32      0.23      0.27       784\n",
      "       B-gpe       0.29      0.36      0.32       346\n",
      "       B-per       0.26      0.47      0.33       410\n",
      "       I-geo       0.53      0.42      0.47       165\n",
      "       B-org       0.20      0.26      0.23       363\n",
      "       I-org       0.11      0.50      0.19       310\n",
      "       B-tim       0.79      0.53      0.63       390\n",
      "       B-art       0.00      0.00      0.00         7\n",
      "       I-art       0.00      0.00      0.00         9\n",
      "       I-per       0.18      0.46      0.26       420\n",
      "       I-gpe       0.00      0.00      0.00         4\n",
      "       I-tim       0.19      0.42      0.26       125\n",
      "       B-nat       0.00      0.00      0.00         2\n",
      "       B-eve       0.40      0.31      0.35        13\n",
      "       I-eve       0.15      0.31      0.21        13\n",
      "       I-nat       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     21778\n",
      "   macro avg       0.25      0.30      0.26     21778\n",
      "weighted avg       0.81      0.74      0.77     21778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# assess performance\n",
    "\n",
    "states = list(ner_y_train.unique()) # list of unique states in our dataset\n",
    "hmm_preds_concat = [item for sublist in hmm_preds for item in sublist]\n",
    "\n",
    "print(classification_report(y_true = ner_y_test, y_pred = hmm_preds_concat, labels = states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are better than our baseline model but only slightly. This can be at least partially attributed to the fact that we had to add default probabilities (1/1000 in this case) for a large fraction of the word/tag combinations since they did not appear in our training data. Therefore, in order to improve performance in the future, we would want to train our model on substantially more data so that the training vocabulary is more expansive and representative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HMMs is generaltive model\n",
    "* CRFs is discriminitive model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 CRFs\n",
    "Lastly, we will implement a Conditional Random Field, or CRF for short. Contrary to HMMs, CRFs are a discriminative model with more relaxed assumptions of the features since we do not need to model the joint probability, P(X,Y) where X=states and y=labels. This means we can utilize more features in our models including past and future observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training data into list of sentences with each row as a tuple of the word, pos and tag\n",
    "\n",
    "sentences_dict_train = {}\n",
    "\n",
    "for index, row in ner_train.iterrows():\n",
    "    if row['Sentence #'] not in sentences_dict_train:\n",
    "        sentences_dict_train[row['Sentence #']] = []\n",
    "    sentences_dict_train[row['Sentence #']].append((row['Word'], row['POS'], row['Tag']))\n",
    "    \n",
    "sentences_train = sentences_dict_train.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "\n",
    "# source: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    # CRF can take featrues in: i.e. consider the lower case and upper case \n",
    "    # (which is needed for some Captial-meanningful cases)\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "X_train = [sent2features(s) for s in sentences_train] # converting training features\n",
    "y_train = [sent2labels(s) for s in sentences_train] # converting tags\n",
    "\n",
    "print(len(X_train)) # check\n",
    "print(len(y_train)) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "crf = sklearn_crfsuite.CRF() # initalize the model\n",
    "\n",
    "# \n",
    "try:\n",
    "    crf.fit(X_train, y_train) # fit the model to the training data\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# transform testing data into list of sentences with each row as a tuple of the word, pos and tag\n",
    "\n",
    "sentences_dict_test = {}\n",
    "\n",
    "for index, row in ner_test.iterrows():\n",
    "    if row['Sentence #'] not in sentences_dict_test:\n",
    "        sentences_dict_test[row['Sentence #']] = []\n",
    "    sentences_dict_test[row['Sentence #']].append((row['Word'], row['POS'], row['Tag']))\n",
    "    \n",
    "sentences_test = sentences_dict_test.values()\n",
    "\n",
    "X_test = [sent2features(s) for s in sentences_test]\n",
    "y_test = [sent2labels(s) for s in sentences_test]\n",
    "\n",
    "print(len(X_test)) # check\n",
    "print(len(y_test)) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.99      0.99      0.99     18417\n",
      "       B-geo       0.77      0.80      0.78       784\n",
      "       B-gpe       0.81      0.85      0.83       346\n",
      "       B-per       0.78      0.75      0.77       410\n",
      "       I-geo       0.75      0.59      0.66       165\n",
      "       B-org       0.64      0.60      0.61       363\n",
      "       I-org       0.71      0.64      0.67       310\n",
      "       B-tim       0.90      0.73      0.81       390\n",
      "       B-art       0.00      0.00      0.00         7\n",
      "       I-art       0.00      0.00      0.00         9\n",
      "       I-per       0.75      0.95      0.83       420\n",
      "       I-gpe       0.00      0.00      0.00         4\n",
      "       I-tim       0.89      0.50      0.64       125\n",
      "       B-nat       0.00      0.00      0.00         2\n",
      "       B-eve       1.00      0.31      0.47        13\n",
      "       I-eve       1.00      0.31      0.47        13\n",
      "       I-nat       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     21778\n",
      "   macro avg       0.59      0.47      0.50     21778\n",
      "weighted avg       0.95      0.95      0.95     21778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jiaqi\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# make predictions & assess performance\n",
    "\n",
    "crf_preds = crf.predict(X_test) \n",
    "crf_preds = [item for sublist in crf_preds for item in sublist] # flatten out predictions\n",
    "\n",
    "print(classification_report(y_true = ner_y_test, y_pred = crf_preds, labels = states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the CRF model performs the best! Let's see if we can improve performance even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HMM can know what happens in the backscence\n",
    "* CRFs only do the better clasification, 各有长处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE: play around with the CRF features/parameters - can you improve performance even more?\n",
    "\n",
    "### For inspiration, check out https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-use-conll-2002-data-to-build-a-ner-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example model parameters (try different for differnt ideas and context)\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=200,\n",
    "    c2=0.1,\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "video-info": {
   "admin": [
    "localhost:8888"
   ],
   "hub-user": "localhost:8888",
   "path": "Labs>>Lab 5 - Sequence Mining/Lab4_Sequences-StarterCode.ipynb",
   "tpc": "localhost:8888@Labs>>Lab 5 - Sequence Mining/Lab4_Sequences-StarterCode.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
